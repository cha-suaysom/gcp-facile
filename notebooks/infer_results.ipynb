{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FACILE Inference\n",
    "The following notebook connects to GCP server and perform inference on the input files with the existing weight file in the server. The two main repositories that this code is based on are\n",
    "1. This file is a client-side of a slightly modified (hard code argument rather than from command lines) [gcp-facile client](https://github.com/cha-suaysom/gcp-facile/blob/master/client.py) \n",
    "2. The server side is written in [gcp-facile server](https://github.com/cha-suaysom/gcp-facile/blob/master/server.py) and is hosted on the Kubernetes services in HarrisGroup GCP.\n",
    "3. The algorithm is from Jeff's [FACILE](https://github.com/JackDinsmore/FACILE/blob/master/train-models.py)\n",
    "\n",
    "The file `weights.h5` is obtained from training `X_HB,Y_HB` that can be obtained from this [drive](https://drive.google.com/drive/folders/0AIBNryPDLt0QUk9PVA). We ran `train_model.py` using model `Model4ExpLLLow` on these data to obtain the weight file which is used for the inference below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input for the client\n",
    "1. The file `input/X_HB.pkl` downloaded from the drive above.\n",
    "2. Proto file `server_tools.proto` has to be in the same directory and compiled with `python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. server-tools.proto`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import grpc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import server_tools_pb2\n",
    "import server_tools_pb2_grpc\n",
    "from google.protobuf import empty_pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server Parameter\n",
    "The following is port and IP address for the Kubernetes service.\n",
    "Please note that this may change when the server is restart or removed and please refer to the services in GCP in `HarrisGroup` for latest update. For this particular notebook the server is hosted [here](https://console.cloud.google.com/kubernetes/service/us-west1-a/cha-facile/default/inference-facile-service?project=harrisgroup-223921&cloudshell=false&tab=overview&duration=PT1H&pod_summary_list_tablesize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PORT = '50051'\n",
    "IP = \"34.82.75.201\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Function\n",
    "This function connects to the server with the specified port/IP and send the pandas loaded from `input/X_HB.pkl` to the server with the existing weight file `weights.h5`. It receives prediction value from the server and time that the prediction uses. `Nrhs` is the number of data points sent to the server for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_facile(host_IP,Nrhs):\n",
    "    # Get a handle to the server\n",
    "    channel = grpc.insecure_channel(host_IP + ':' + PORT)\n",
    "    stub = server_tools_pb2_grpc.MnistServerStub(channel)\n",
    "\n",
    "    # Get a client ID which you need to talk to the server\n",
    "    try:\n",
    "        logging.info(\"Request client id from server\")\n",
    "        response = stub.RequestClientID(server_tools_pb2.NullParam())\n",
    "    except BaseException:\n",
    "        print(\n",
    "             \"\"\"Connection to the server could not be established.\n",
    "             Press enter to try again.\"\"\")\n",
    "        return\n",
    "    client_id = response.new_id\n",
    "    logging.info(\"Client id is \" + str(client_id))\n",
    "\n",
    "    X = pd.read_pickle(\"input/X_HB.pkl\")[:Nrhs]\n",
    "    Y = pd.read_pickle(\"input/Y_HB.pkl\")[:Nrhs]\n",
    "\n",
    "    x_input = X.to_json().encode('utf-8')\n",
    "    y_input = Y.to_json().encode('utf-8')\n",
    "\n",
    "    # Send the data to the server and receive an answer\n",
    "    start_time = time.time()\n",
    "    logging.info(\"Submitting images and waiting\")\n",
    "    data_message = server_tools_pb2.DataMessage(\n",
    "        client_id=client_id, x_input=x_input, y_input=y_input, batch_size=32)\n",
    "    logging.info(\"Finish defining data message\")\n",
    "    response = stub.StartJobWait(data_message, 100, [])\n",
    "    logging.info(\"Finish responding\")\n",
    "\n",
    "    # Print output\n",
    "    whole_time = time.time() - start_time\n",
    "    print(\"Total time:\", whole_time)\n",
    "    print(\"Predict time:\", response.infer_time)\n",
    "    print(\"Fraction of time spent not predicting:\",\n",
    "          (1 - response.infer_time / whole_time) * 100, '%')\n",
    "    \n",
    "    # Sample of the inference value to see if it makes sense\n",
    "    print(np.frombuffer(response.prediction,dtype = np.float32)[:10])\n",
    "    channel.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample connections to the server based on different input size\n",
    "\n",
    "For details on what happens at the server side. Please refer to the [Container Logs](https://console.cloud.google.com/logs/viewer?interval=NO_LIMIT&project=harrisgroup-223921&cloudshell=false&minLogLevel=0&expandAll=false&timestamp=2019-10-16T22:41:41.858000000Z&customFacets=&limitCustomFacetWidth=true&advancedFilter=resource.type%3D%22container%22%0Aresource.labels.cluster_name%3D%22cha-facile%22%0Aresource.labels.namespace_id%3D%22default%22%0Aresource.labels.project_id%3D%22harrisgroup-223921%22%0Aresource.labels.zone:%22us-west1-a%22%0Aresource.labels.container_name%3D%22attempt-deploy-sha256%22%0Aresource.labels.pod_id:%22infer-facile-%22&scrollTimestamp=2019-10-16T21:33:19.569536608Z)\n",
    "I observe the following\n",
    "1. The \"total time\" is quite random. Sometimes it's just my internet connection and how much RAM I'm using at that moment. Preprocessing time on the server-side may be unpredictable as well.\n",
    "2. On the other hand, the \"Predict Time\" (time spent in Keras predict function) has quite a clear increasing pattern as a function of number of input rows. There may be other interesting measurable time usage that may be helpful.\n",
    "3. The prediction for the first 10 data point is quite consistent. However, I don't know if this is accurate (will discuss with FACILE folks to see if there is a good way to check this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 1.8064167499542236\n",
      "Predict time: 0.14994049072265625\n",
      "Fraction of time spent not predicting: 91.6995626437556 %\n",
      "[ 0.13345513  0.60735834 32.12213     0.8409045   4.3819866   3.9754093\n",
      "  0.12074747 23.26381     2.8654158   0.13345513]\n"
     ]
    }
   ],
   "source": [
    "run_facile(IP,500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 1.3543760776519775\n",
      "Predict time: 0.1409001350402832\n",
      "Fraction of time spent not predicting: 89.59667574130845 %\n",
      "[ 0.13345513  0.4460571  34.881256    0.60553753  4.488502    4.2128572\n",
      "  0.13345513 25.097042    3.0467412   0.13345513]\n"
     ]
    }
   ],
   "source": [
    "run_facile(IP,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 1.4381999969482422\n",
      "Predict time: 0.17693853378295898\n",
      "Fraction of time spent not predicting: 87.69722332370951 %\n",
      "[ 0.16506827  0.88802063 30.557487    0.9523132   4.1341715   3.8466966\n",
      "  0.18208459 22.052444    3.282159    0.13345513]\n"
     ]
    }
   ],
   "source": [
    "run_facile(IP,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 5.983857870101929\n",
      "Predict time: 0.2187633514404297\n",
      "Fraction of time spent not predicting: 96.34410849673635 %\n",
      "[ 0.13345513  0.74879324 31.342875    0.821262    4.1955395   3.913856\n",
      "  0.13345513 22.566328    3.164562    0.13345513]\n"
     ]
    }
   ],
   "source": [
    "run_facile(IP,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time: 21.985822916030884\n",
      "Predict time: 0.273975133895874\n",
      "Fraction of time spent not predicting: 98.75385545065905 %\n",
      "[ 0.12786819  0.74560845 31.920801    0.83843553  4.2515993   3.9767528\n",
      "  0.13296703 23.08878     3.2147532   0.13345513]\n"
     ]
    }
   ],
   "source": [
    "run_facile(IP,5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data size limitation\n",
    "Lastly, there is a limit (somewhere around 7000 data points) and 10000 just exceeds the limit so the server throws an error (I'm working on preventing this). Anything significantly beyond this may need to be uploaded to the cloud and read from there instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "_Rendezvous",
     "evalue": "<_Rendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNKNOWN\n\tdetails = \"Exception calling application: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(12, 36), dtype=float32) is not an element of this graph.\"\n\tdebug_error_string = \"{\"created\":\"@1571261670.010468200\",\"description\":\"Error received from peer\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1036,\"grpc_message\":\"Exception calling application: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(12, 36), dtype=float32) is not an element of this graph.\",\"grpc_status\":2}\"\n>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_Rendezvous\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-e1ec6734cb10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_facile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-26-cd415f3d8a30>\u001b[0m in \u001b[0;36mrun_facile\u001b[1;34m(host_IP, Nrhs)\u001b[0m\n\u001b[0;32m     28\u001b[0m         client_id=client_id, x_input=x_input, y_input=y_input, batch_size=32)\n\u001b[0;32m     29\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Finish defining data message\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStartJobWait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Finish responding\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/root/anaconda3/lib/python3.5/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready)\u001b[0m\n\u001b[0;32m    545\u001b[0m         state, call, = self._blocking(request, timeout, metadata, credentials,\n\u001b[0;32m    546\u001b[0m                                       wait_for_ready)\n\u001b[1;32m--> 547\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     def with_call(self,\n",
      "\u001b[1;32m/root/anaconda3/lib/python3.5/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[1;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[0;32m    464\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 466\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0m_Rendezvous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeadline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31m_Rendezvous\u001b[0m: <_Rendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNKNOWN\n\tdetails = \"Exception calling application: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(12, 36), dtype=float32) is not an element of this graph.\"\n\tdebug_error_string = \"{\"created\":\"@1571261670.010468200\",\"description\":\"Error received from peer\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1036,\"grpc_message\":\"Exception calling application: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"Placeholder:0\", shape=(12, 36), dtype=float32) is not an element of this graph.\",\"grpc_status\":2}\"\n>"
     ]
    }
   ],
   "source": [
    "run_facile(IP,10000)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
