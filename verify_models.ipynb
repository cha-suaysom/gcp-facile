{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, BatchNormalization, Input, Dropout, Activation, concatenate, GRU, Dropout\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adam, Nadam, SGD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, threading\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Disable depreciation warnings and limit verbosity during training\n",
    "from tensorflow.python.util import deprecation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATE = 0.001\n",
    "NUM_EPOCHS = 11\n",
    "VALSPLIT = 0.0\n",
    "np.random.seed(5)\n",
    "TRAIN_BATCH_SIZE = 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_pickle(\"input/X_HB.pkl\")[:]\n",
    "X.drop(['PU','pt'],1,inplace=True)\n",
    "mu,std = np.mean(X, axis=0), np.std(X, axis=0)\n",
    "X = (X-mu)/std\n",
    "Y = pd.read_pickle(\"input/Y_HB.pkl\")[:][['genE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/control_flow_ops.py:423: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "inputs_keras = Input(shape=(X.shape[1],), name='input')\n",
    "h = Dropout(rate=RATE)(inputs_keras)\n",
    "h = Dense(36, activation='relu')(h)\n",
    "norm = Dropout(rate=RATE)(h)\n",
    "h = Dense(11, activation = 'relu')(norm)\n",
    "norm = Dropout(rate=RATE)(h)\n",
    "h = Dense(3, activation = 'relu')(norm)\n",
    "net_keras = Dense(1, activation='linear', name='output')(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_keras = Model(inputs=inputs_keras, outputs=net_keras)\n",
    "model_keras.compile(optimizer=Adam(), loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/11\n",
      "574212/574212 [==============================] - 2s 3us/step - loss: 49.9268\n",
      "Epoch 2/11\n",
      "574212/574212 [==============================] - 2s 3us/step - loss: 1.5127\n",
      "Epoch 3/11\n",
      "574212/574212 [==============================] - 2s 3us/step - loss: 1.1050\n",
      "Epoch 4/11\n",
      "574212/574212 [==============================] - 2s 3us/step - loss: 0.8999\n",
      "Epoch 5/11\n",
      "574212/574212 [==============================] - 2s 3us/step - loss: 0.7749\n",
      "Epoch 6/11\n",
      "574212/574212 [==============================] - 2s 3us/step - loss: 0.7372\n",
      "Epoch 7/11\n",
      "574212/574212 [==============================] - 2s 3us/step - loss: 0.6835\n",
      "Epoch 8/11\n",
      "574212/574212 [==============================] - 2s 3us/step - loss: 0.6318\n",
      "Epoch 9/11\n",
      "574212/574212 [==============================] - 2s 3us/step - loss: 0.6938\n",
      "Epoch 10/11\n",
      "574212/574212 [==============================] - 2s 3us/step - loss: 0.6242\n",
      "Epoch 11/11\n",
      "574212/574212 [==============================] - 2s 3us/step - loss: 0.5875\n",
      "[[ 0.29250067]\n",
      " [ 1.0580821 ]\n",
      " [31.831917  ]\n",
      " [ 1.0676107 ]\n",
      " [ 4.4221406 ]\n",
      " [ 4.46185   ]\n",
      " [ 0.2952334 ]\n",
      " [22.562683  ]\n",
      " [ 2.8525708 ]\n",
      " [ 0.3161993 ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model_keras.fit(X.values, Y.values,batch_size=2024, epochs=NUM_EPOCHS, shuffle=True,validation_data=(X, Y))\n",
    "idx = np.random.permutation(X.shape[0])\n",
    "train_idx = idx[int(VALSPLIT*len(idx)):]\n",
    "val_idx = idx[:int(VALSPLIT*len(idx))]\n",
    "# model_keras.fit(X.values[train_idx], Y.values[train_idx],batch_size=2024, \n",
    "#                 epochs=NUM_EPOCHS, shuffle=True, validation_data = (X.values[val_idx], Y.values[val_idx]))\n",
    "\n",
    "model_keras.fit(X.values[train_idx], Y.values[train_idx],batch_size=TRAIN_BATCH_SIZE, \n",
    "                epochs=NUM_EPOCHS, shuffle=True)#, validation_data = (X.values[val_idx], Y.values[val_idx]))\n",
    "\n",
    "result = model_keras.predict(X, 10000)\n",
    "print(result[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = 'gs://harrisgroup-ctpu/facile/model/'\n",
    "DATA_DIR = 'gs://harrisgroup-ctpu/facile/data/'\n",
    "TPU_NAME='uw-tpu'\n",
    "ZONE_NAME='us-central1-b'\n",
    "PROJECT_NAME = 'harrisgroup-223921'\n",
    "NUM_ITERATIONS = 100000 # Number of iterations per TPU training loop\n",
    "TRAIN_STEPS = 70000\n",
    "NUM_SHARDS = 8 # Number of shards (TPU chips).\n",
    "USE_TPU = True\n",
    "\n",
    "lock = threading.Lock()\n",
    "\n",
    "#tf.enable_eager_execution()\n",
    "\n",
    "# DEFINE THE NETWORK\n",
    "\n",
    "class ModelFacile(object):\n",
    "    def __call__(self, inputs):\n",
    "        net = tf.layers.dropout(inputs, rate = 0.001)\n",
    "        net = tf.layers.dense(net, 36, activation = 'relu')\n",
    "        norm = tf.layers.dropout(net, rate = 0.001)\n",
    "        net = tf.layers.dense(norm, 11, activation = 'relu')\n",
    "        norm = tf.layers.dropout(net, rate = 0.001)\n",
    "        net = tf.layers.dense(norm, 3, activation = 'relu')\n",
    "        return tf.layers.dense(net, 1, activation = 'linear', name='output')\n",
    "\n",
    "\n",
    "def input_facile(train_x,train_y,batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((tf.cast(train_x,tf.float32),tf.cast(train_y,tf.float32)))\n",
    "    return dataset.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "    model = ModelFacile()\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predicted_values = model(features)\n",
    "        #print(\"Shape of predicted values is \", tf.shape(predicted_values))\n",
    "        predictions = {\n",
    "            'probabilities': predicted_values,\n",
    "        }\n",
    "        return tf.contrib.tpu.TPUEstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    predicted_values = model(features)\n",
    "    loss = tf.losses.mean_squared_error(labels=labels,predictions=predicted_values)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      optimizer = tf.train.AdagradOptimizer(learning_rate = 0.1)\n",
    "      if USE_TPU:\n",
    "        optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
    "      train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "      return tf.contrib.tpu.TPUEstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "\n",
    "# CREATE AND PREDICT WITH TPUS\n",
    "\n",
    "def create_estimator(batch_size):\n",
    "    print(\"Creating the estimator\")\n",
    "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "        TPU_NAME,\n",
    "        zone=ZONE_NAME,\n",
    "        project=PROJECT_NAME)\n",
    "\n",
    "    run_config = tf.contrib.tpu.RunConfig(\n",
    "        cluster=tpu_cluster_resolver,\n",
    "        model_dir=MODEL_DIR,\n",
    "        session_config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True),\n",
    "        tpu_config=tf.contrib.tpu.TPUConfig(NUM_ITERATIONS, NUM_SHARDS))\n",
    "\n",
    "    estimator = tf.contrib.tpu.TPUEstimator(\n",
    "        model_fn=model_fn,\n",
    "        use_tpu=USE_TPU,\n",
    "        train_batch_size=batch_size,\n",
    "        eval_batch_size=batch_size,\n",
    "        predict_batch_size=batch_size,\n",
    "        params={\"data_dir\": DATA_DIR},\n",
    "        config=run_config)\n",
    "    return estimator\n",
    "\n",
    "def predict(data, batch_size,estimator, results=None, times=None, job_id=None):\n",
    "    assert (results is None and times is None and job_id is None) or not (results is None or times is None or job_id is None)\n",
    "    lock.acquire()\n",
    "    \n",
    "\n",
    "    print(\"Predicting\")\n",
    "    def predict_input_fn(params):\n",
    "        batch_size = params[\"batch_size\"]\n",
    "        dataset_predict = tf.data.Dataset.from_tensor_slices(data.astype('float32'))\n",
    "        return dataset_predict.batch(batch_size)\n",
    "\n",
    "    predictions = estimator.predict(predict_input_fn)\n",
    "    \n",
    "    num_print = 10\n",
    "    num = 0\n",
    "    predicted_values = []\n",
    "    start_time = time.time()\n",
    "    for pred_dict in predictions:\n",
    "        #print(pred_dict.keys())\n",
    "        #print(pred_dict.values())\n",
    "        break\n",
    "        #predicted_values.append(list(pred_dict.values())[0][0])\n",
    "        #num += 1\n",
    "#         if (num >= num_print):\n",
    "#             break\n",
    "    #predicted_values =np.array(predicted_values)/np.sum(predicted_values)\n",
    "    #print(\"Number of prediction is \",i)\n",
    "    #print(predicted_values[:10])\n",
    "\n",
    "    predict_time = time.time() - start_time\n",
    "    lock.release()\n",
    "\n",
    "    return predicted_values[:10],predict_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator = create_estimator(2024)\n",
    "#estimator.train(input_fn=lambda params: input_facile(X,Y,params[\"batch_size\"]),max_steps=TRAIN_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the estimator\n",
      "Predicting\n",
      "Time used in the prediction is  4.95343542098999\n",
      "Predicting\n",
      "Time used in the prediction is  8.588505983352661\n",
      "Predicting\n",
      "Time used in the prediction is  9.893390655517578\n",
      "Predicting\n",
      "Time used in the prediction is  16.089070796966553\n"
     ]
    }
   ],
   "source": [
    "time_list = []\n",
    "tf.logging.set_verbosity(tf.logging.FATAL)\n",
    "estimator = create_estimator(100000)\n",
    "for N in [1000,10000,100000,600000]:\n",
    "    start_time = time.time()\n",
    "    predicted_values = predict(X[:N],1,estimator)\n",
    "    time_in_s = time.time() - start_time\n",
    "    print(\"Time used in the prediction is \", time_in_s)\n",
    "    time_list.append(time_in_s/N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00495343542098999, 0.0008588505983352661, 9.893390655517579e-05, 2.6815117994944256e-05]\n"
     ]
    }
   ],
   "source": [
    "print(time_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00014641989707946777"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.00019141989707946778*600000-27)/600000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.013894327163696289,\n",
       " 0.004109413719177246,\n",
       " 0.0014968007469177247,\n",
       " 0.0010716665017604828]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[13.894327163696289/1000, 41.09413719177246/10000, 149.68007469177246/100000, 642.9999010562897/600000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. TRAIN_STEPS*TRAIN_BATCH_SIZE < len(X)\n",
    "2. Wrong learning rate kills everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "print(np.array(Y[\"genE\"][:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
